{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers a la HuggingFace "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers can be used in several areas like:\n",
    "- Natural Language Processing (NLP)\n",
    "- Computer Vision (CV)\n",
    "- Automatic Speech Recognition (ASR)\n",
    "\n",
    "But this notebook we will focus on NLP and showcase a number of possible tasks like:\n",
    "- Text Generation,\n",
    "- Translation,\n",
    "- Summarisation,\n",
    "- and more.\n",
    "\n",
    "That you easelly can perform by youself using the *transformers* packet and open models made available by HuggingFace.\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The Notebook will cover\n",
    "\n",
    "HuggingFace Components:\n",
    "- **Tokenizer**: Maps text (string) to tokens and associated id (int) that can be understood by a model. ([Tokenizer summary](https://huggingface.co/docs/transformers/tokenizer_summary))\n",
    "- **Model**: A transformer model \n",
    "- **Pipeline**: Putting a tokenizer and model together for easy use  \n",
    "\n",
    "Terminology:\n",
    "- **Prompt**: Text input to a generative model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some necessary installations for your Colab instance\n",
    "! pip install transformers\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation - Distilled GPT2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating a (pretrained) model and its tokenizer form the HuggingFace Zoo. In this case we choose a distilled version of GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make it easy with a completed pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the tokenizer & the model in a pipeline\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define a pretty printing function :)\n",
    "def print_res(result, prompt):\n",
    "    i = 1\n",
    "    for x, y in zip(result, prompt):\n",
    "        j = 1\n",
    "        print(f\"--- Prompt {i} ---: \")\n",
    "        print(\">> Prompt: \", y, \"...\")\n",
    "        for xx in x:\n",
    "            print(f\">> Output ({j}): \", xx[\"generated_text\"][len(y):])\n",
    "            j += 1\n",
    "        print(\"------------------\\n\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "        # General\n",
    "        \"pad_token_id\": 50256,\n",
    "        \"max_length\": 50, \n",
    "        \"no_repeat_ngram_size\": 2, \n",
    "        \"repetition_penalty\": 1, \n",
    "        \"num_return_sequences\": 2,\n",
    "\n",
    "        # # Beam search\n",
    "        # \"num_beams\": 5, \n",
    "        # \"num_return_sequences\": 2,\n",
    "        # \"early_stopping\": True,\n",
    "        \n",
    "        # # Sampling\n",
    "        # \"temperature\": 1,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 0,\n",
    "        \"max_length\": 50, \n",
    "        \"top_p\": 0.92, \n",
    "        }\n",
    "\n",
    "\n",
    "prompt = [\n",
    "    \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\",\n",
    "    \"Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne\",\n",
    "    \"I'm a Transformer and welcome to my TED-talk\",\n",
    "    ]\n",
    "\n",
    "result = generator(\n",
    "    prompt,\n",
    "    **settings\n",
    ")\n",
    "\n",
    "print_res(result, prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox - Try to play around with different prompts, settings and models!\n",
    "\n",
    "#### Models\n",
    "Text Generation models compatiable with the pipeline can be found [here](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)\n",
    "- For example [lunde/gpt2-snapsvisor](https://huggingface.co/lunde/gpt2-snapsvisor) a *finetuned swedish version* of *GPT2* that writes ***snapsvisor***... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and create pipeline\n",
    "# This might take a few minutes for a new model! So a tip is to load a model, and then play around with prompts and settings in the next cell\n",
    "\n",
    "# Can be changed to any model name found on HuggingFace this link: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads \n",
    "model_name = 'lunde/gpt2-snapsvisor'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Settings\n",
    "Text generation and its settings [explained by HuggingFace](https://huggingface.co/blog/how-to-generate), includes example settings for \n",
    "- Greedy/Beam search \n",
    "- Sampling strategies\n",
    "\n",
    "Full [list of settings](https://huggingface.co/docs/transformers/main_classes/text_generation) for the generate function, some exampels\n",
    "- ***min_length*** & ***max_length***: (int) min/max tokens to generate\n",
    "- ***no_repeat_ngram_size***: (int) constrain the repetitiveness of the generation. (3 -> no three word sequence can be repeted)\n",
    "- ***repetition_penalty***: (float) penatlty factor for repetion (1 means no penalty)\n",
    "\n",
    "Rerun the cell with different settings and prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change prompt and settings, and generate output\n",
    "\n",
    "# A dictionary with settings for the generation function\n",
    "settings = {\n",
    "        # General\n",
    "        \"pad_token_id\": 50256,\n",
    "        \"max_length\": 50, \n",
    "        # \"no_repeat_ngram_size\": 2, \n",
    "        \"repetition_penalty\": 1, \n",
    "        \n",
    "        # # Beam search\n",
    "        # \"num_beams\": 5, \n",
    "        # \"num_return_sequences\": 2,\n",
    "        # \"early_stopping\": True,\n",
    "        \n",
    "        # # Sampling\n",
    "        # \"temperature\": 1,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 0,\n",
    "        \"max_length\": 50, \n",
    "        \"top_p\": 0.92, \n",
    "        }\n",
    "\n",
    "# List containing all strings you would like to send to the model as prompts \n",
    "prompt = [\n",
    "    \"Tre ringar för älvkungarnas makt högt i det blå, sju för dvärgarnas furstar i salarna av sten\",\n",
    "    \"nio för de dödliga som köttets väg ska gå, en för Mörkrets herre i ondskans dunkla sken\",\n",
    "    ]\n",
    "\n",
    "# Storing the result \n",
    "result = generator(\n",
    "    prompt,\n",
    "    **settings\n",
    ")\n",
    "\n",
    "print_res(result, prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's going on inside the pipeline?\n",
    "For the interested person. Here we can see how the pipeline works with tensors, encodings and decodings to go from input to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Input Prompt\n",
    "text_input = \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\"\n",
    "input_length = len(text_input)\n",
    "\n",
    "# Encoding the prompt\n",
    "input_ids = tokenizer.encode(text_input, return_tensors=\"pt\")\n",
    "\n",
    "# Generating\n",
    "output_ids = model.generate(input_ids, do_sample=True, top_k=0, max_length=50, top_p=0.92, pad_token_id=50256)\n",
    "\n",
    "# Decoding the output\n",
    "text_output = tokenizer.batch_decode(output_ids)\n",
    "\n",
    "# Checking the result\n",
    "print(\">> Prompt: \", text_input, \"...\")\n",
    "print(\">> Output: \", text_output[0][input_length:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = input_ids[:10].tolist()\n",
    "for token, id in zip(tokenizer.tokenize(text_input), input_ids.tolist()[0]):\n",
    "    print(f\"{id}:\\t {token.replace('Ġ', '_')}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification (examplified through sentiment analysis)\n",
    "In text classification we want to categorize our text. A common example is _sentiment analysis_, or classifying if a text is positive or negative. This is partly because there are a lot of labeled data available online in the form of reviews, we we quite easily can get a lot of training data where we know that a 1-star review probably has a negative tone for example.\n",
    "\n",
    "\n",
    "Some other examples of where text-classification can be useful are: detecting the language of a text, classifying spam, finding urgency and important in customer messages or detecting toxic messages. \n",
    "\n",
    "[Here are other classifcation models on HuggingFace](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads). Most are sentiment analysis models, usually trained on either twitter or movie reviews from imdb. There are also other types of classification models here if you dig a bit, for example toxicity models. You can ask Victor if you want to know more about toxicity models specificly :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = \"This movie is disgustingly good !\"\n",
    "review2 = \"Director tried too much.\"\n",
    "\n",
    "print(f'>>{review1}<< {classifier(review1)}\\n>>{review2}<< {classifier(review2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot-Classification "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a large pretrained NLP model to classify text into never seen classes i.e. ***zero-shot***\n",
    "\n",
    "Models [avaialble](https://huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "oracle = pipeline(model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oracle(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    "))\n",
    "\n",
    "oracle(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"english\", \"german\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oracle(\n",
    "    \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\",\n",
    "    candidate_labels=[\"upper-class\", \"middle-class\", \"lower-class\"],\n",
    "))\n",
    "oracle(\n",
    "    \"Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne\",\n",
    "    candidate_labels=[\"happy\", \"sad\", \"epic\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-2-Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a Text to Text transformer trained to generate a question that fits to a given answer and context. \n",
    "\n",
    "Other models [avaialble](https://huggingface.co/models?pipeline_tag=text2text-generation&sort=downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Generator Pipeline\n",
    "generator = pipeline(model=\"mrm8488/t5-base-finetuned-question-generation-ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context and a sought answer\n",
    "context = \"Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n",
    "answer = \"Manuel\"\n",
    "\n",
    "result = generator(f\"answer: {answer} context: {context}\")[0][\"generated_text\"]\n",
    "\n",
    "print(\"--- Input ---\")\n",
    "print(\"Context:\", context)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"--- Output ---\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the context and the answer you would like and see what question the model can come up with!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne\"\n",
    "\n",
    "answer = \"stone halls\"\n",
    "\n",
    "result = generator(f\"answer: {answer} context: {context}\")[0][\"generated_text\"]\n",
    "\n",
    "print(\"--- Input ---\")\n",
    "print(\"Context:\", context)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"--- Output ---\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation models avaiable on HuggingFace compatiable with [pipeline](https://huggingface.co/models?pipeline_tag=translation&sort=downloads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "# The T5 model supports the following languages: en, de, fr & ro \n",
    "# Change the language by switching xx & yy in translation_xx_to_yy  \n",
    "en_de_translator = pipeline(\"translation_en_to_de\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_de_translator(\"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational AI\n",
    "Using a large pretrained NLP model to have a conversation. These models are usually trained to be __engaging__, whatever that means. \n",
    "The script below runs starts a conversation with an _initial_prompt_ and then runs for 5 turns, where you can reply to the model.\n",
    "\n",
    "You can try out different prompts, models and replies. Maybe compare to a purely generative model, that you ask to simulate a conversation.\n",
    "\n",
    "Models [avaialble](https://huggingface.co/models?pipeline_tag=conversational&sort=downloads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation\n",
    "\n",
    "chatbot = pipeline(task='conversational', model ='facebook/blenderbot-400M-distill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_input = \"Going to the movies tonight - any suggestions?\"\n",
    "conversation = Conversation(intial_input)\n",
    "reply = chatbot(conversation)\n",
    "print(reply)\n",
    "\n",
    "# run a conversation for 5 turns\n",
    "for step in range(5):\n",
    "    new_input = input(\">>User:\")\n",
    "    conversation.add_user_input(new_input)\n",
    "    reply = chatbot(conversation)\n",
    "    print(reply)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "Models trained to extract __Entities__, i.e. people, places, organizations and so on. They are usually tagged with: \n",
    "|  |  |\n",
    "|---|---|\n",
    "| ORG | organization |\n",
    "| LOC | location |\n",
    "| PER | person |\n",
    "| MISC | miscellaneous |\n",
    "\n",
    "They also get prefixes, either I- or O-. I- is the most common, O- is used for to distinguish between entities if there are several of the same tag directly after each other. This is how to interpret the full tags:\n",
    "| Abbreviation | Description |\n",
    "|---|---|\n",
    "|O |\tOutside of a named entity|\n",
    "|B-MIS| \tBeginning of a miscellaneous entity right after another miscellaneous entity|\n",
    "|I-MIS| \tMiscellaneous entity|\n",
    "|B-PER| \tBeginning of a person’s name right after another person’s name|\n",
    "|I-PER| \tPerson’s name|\n",
    "|B-ORG| \tBeginning of an organization right after another organization|\n",
    "|I-ORG| \torganization|\n",
    "|B-LOC| \tBeginning of a location right after another location|\n",
    "|I-LOC| \tLocation|\n",
    "\n",
    "\n",
    "The most common model on HuggingFace is in frech for some reason, probably because it has a nice name - camemBERT. \n",
    "The default model is an English version of BERT, finetuned for NER. You can find other models [here](https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads). This is an example of a _token classification task_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(\"ner\")\n",
    "# calling the pipeline without an argument results in it loading a default mode. In this case, this is equivalent to:\n",
    "# ner_model = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "#ner_pipe = pipeline(task=\"token-classification\", model=ner_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence = \"\"\"NordAxon is a company based in Malmö, currently Filip and Victor are in Halmstad with HighFive Halmstad\"\"\"\n",
    "\n",
    "for entity in ner_pipe(sequence):\n",
    "    print(entity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "A question answering model takes two input parameters: a _context_ and a _question_. It tries to answer the _question_ using information in the _context_.\n",
    "You could for example pipe in an article as a context, and ask questions through the model. These models are mainly trained on answering simple fact-based questions.\n",
    "\n",
    "[Other models available here](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\"\n",
    "\n",
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "Models trained to summarize longer texts. These kinds of models are becomming more and more common, especially in AI-newsletters for some reason :thinking_face:\n",
    "\n",
    "Here are [other available models](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads). Many of them are trained on a dataset of news articles from [CNN and the Daily Mail](https://huggingface.co/datasets/cnn_dailymail). This dataset can be downloaded and interactied with through another HuggingFace package - [datasets](https://huggingface.co/docs/datasets/index) if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", \"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the article below is a snapshot of the text in this wikipedia article, as collected on 2023-01-25: https://en.wikipedia.org/wiki/Artificial_Intelligence_Act\n",
    "\n",
    "article = \"\"\"The Artificial Intelligence Act (AI Act) is a regulation[1] proposed on 21 April 2021 by the European Commission which aims to introduce a common regulatory and legal framework for artificial intelligence.[2] Its scope encompasses all sectors (except for military), and to all types of artificial intelligence. As a piece of product regulation, the proposal does not confer rights on individuals, but regulates the providers of artificial intelligence systems, and entities making use of them in a professional capacity.\n",
    "\n",
    "The proposed regulation classifies artificial intelligence applications by risk, and regulates them accordingly. Low-risk applications are not regulated at all, with Member States largely precluded via maximum harmonisation from regulating them further and existing national laws relating to the regulation of design or use of such systems disapplied.[3] A voluntary code of conduct scheme for such low risk systems is envisaged, although not present from the outset. Medium and high-risk systems would require compulsory conformity assessment, undertaken as self-assessment by the provider, before being put on the market. Some especially critical applications which already require conformity assessment to be supervised under existing EU law, for example for medical devices, would the provider's self-assessment under AI Act requirements to be considered by the notified body conducting the assessment under that regulation, such as the Medical Devices Regulation.\n",
    "\n",
    "The proposal also would place prohibitions on certain types of applications, namely remote biometric recognition, applications that subliminally manipulate persons, applications that exploit vulnerabilities of certain groups in a harmful way, and social credit scoring. For the first three, an authorisation regime context of law enforcement is proposed, but social scoring would be banned completely.[4]\n",
    "\n",
    "The act also proposes the introduction of a European Artificial Intelligence Board which will encourage national cooperation and ensure that the regulation is respected.[5]\n",
    "\n",
    "Like the European Union's General Data Protection Regulation (GDPR), the AI Act could become a global standard.[6] It is already having impact beyond Europe; in September 2021, Brazil’s Congress passed a bill that creates a legal framework for artificial intelligence.[7]\n",
    "\n",
    "The European Council adopted its general approach on the AI Act on 6 December 2022.[8] Germany supports the Council's position but still sees some need for further improvement as formulated in an accompanying statement by the member state.[9]\n",
    "\n",
    "The EU AI Act is a proposal by the European Commission to regulate Artificial Intelligence (AI) in the EU. The goal is to create a framework to manage and mitigate risks of AI systems and build trust in them. The proposal includes a classification system for AI systems based on risk level and prioritizes the fundamental rights of individuals. The proposal has undergone changes, such as amendments from the Parliament and the French and Czech presidencies, with the aim to balance between protecting fundamental rights and promoting AI. \"\"\"\n",
    "\n",
    "summarizer(article)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Speech Recognition (ASR) / Speech to text (STT) with Wav2Vec2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import IPython.display as ipd\n",
    "\n",
    "print(\"Audio Backend found:\", torchaudio.get_audio_backend())\n",
    "assert torchaudio.get_audio_backend() != None, \"fail\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a swedish version of Wav2Vec2 ([VoxRex](https://huggingface.co/KBLab/wav2vec2-large-voxrex-swedish)) trained by KBlabs (Kungliga biblioteket) and a dataset Common Voice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"KBLab/wav2vec2-large-voxrex-swedish\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"KBLab/wav2vec2-large-voxrex-swedish\")\n",
    "\n",
    "test_dataset = load_dataset(\"common_voice\", \"sv-SE\", split=\"test[:2%]\")\n",
    "sample_rate = 16000\n",
    "resampler = torchaudio.transforms.Resample(48_000, sample_rate)\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the aduio files as arrays\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n",
    "    return batch\n",
    "\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model to transcribe some audio files and print the predicted transcription as well as the actual transcription (reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test_dataset contains 41 exxamples, change the first/last values below to get different examples\n",
    "first = 0\n",
    "last = 8\n",
    "\n",
    "# Inference/Prediction\n",
    "inputs = processor(test_dataset[\"speech\"][first:last], sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to lisent to the audio samples and verify that the transcription is correct! Change the variable *index* below to lisent other predicted samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(f\"Example {index} of {len(inputs.input_values)} predicted (index [0,..,{len(inputs.input_values)-1}] available).\")\n",
    "print(\"Prediction transcription:\", processor.batch_decode(predicted_ids)[index])\n",
    "print(\"Reference  transcription:\", test_dataset[\"sentence\"][first:last][index])\n",
    "\n",
    "sample = inputs.input_values[index]\n",
    "ipd.Audio(sample, rate=sample_rate)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all predicted examples transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction (model):\\n\", processor.batch_decode(predicted_ids))\n",
    "print(\"Reference:\\n\", test_dataset[\"sentence\"][first:last])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f864bb1b91fb7c90a599af589a79f99bf504df31dafd2732816778f2e2b210"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
